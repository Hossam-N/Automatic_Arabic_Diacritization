{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Tashkeel </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project is developed by Abdulhameed Osama, Hossam Nabil and Nourhan Mohamed as a part of the Natural Language Processing course (NLP) at Cairo University."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project a Bi-LSTM model is trained to predict the diacritics of Arabic text. The model is trained on a dataset of 18 million characters from various domains. The model is trained on Google Colab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We achieved an accuracy of 89.5% on the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model starts with a character embedding layer, followed by a Bi-LSTM layer, then a dense layer and finally a softmax layer. The model is trained using the Adam optimizer and the categorical crossentropy loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Imports </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle as pkl\n",
    "import helper_file as hf\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import re\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arabic Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the dataset\n",
    "dataset = hf.read_file('dataset/train.txt')\n",
    "\n",
    "val = hf.read_file('dataset/val.txt')\n",
    "\n",
    "arabic_letters = hf.read_pickle('Delivery/arabic_letters.pickle')\n",
    "\n",
    "diacritics = hf.read_pickle('Delivery/diacritics.pickle')\n",
    "\n",
    "diacritics_to_id = hf.read_pickle('Delivery/diacritics2id.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Clean the original dataset\n",
    "cleaned_dataset_with_diacritics = hf.clean_dataset(dataset, remove_diacritics=False)\n",
    "cleaned_dataset = hf.clean_dataset(dataset, remove_diacritics=True)\n",
    "hf.write_file('Delivery/cleaned_dataset.txt', cleaned_dataset)\n",
    "hf.write_file('Delivery/cleaned_dataset_with_diacritics.txt', cleaned_dataset_with_diacritics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_val_with_diacritics = hf.clean_dataset(val, remove_diacritics=False)\n",
    "cleaned_val = hf.clean_dataset(val, remove_diacritics=True)\n",
    "hf.write_file('Delivery/cleaned_val.txt', cleaned_val)\n",
    "hf.write_file('Delivery/cleaned_val_with_diacritics.txt', cleaned_val_with_diacritics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_arabic_sentences(text):\n",
    "   \n",
    "    pattern = r'(?<=[.؟!,؛])'\n",
    "\n",
    "    # Split the text into sentences based on the pattern\n",
    "    sentences = re.split(pattern, text)\n",
    "    # remove punctuations\n",
    "    sentences = [re.sub(r'[.؟!،؛]', '', sent) for sent in sentences]\n",
    "    sentences = [re.sub(r'\\s+', ' ', sent) for sent in sentences]\n",
    "    sentences = filter(lambda sentences: sentences.strip(), sentences)\n",
    "\n",
    "    return sentences \n",
    "\n",
    "\n",
    "def append_to_file(file_path, content):\n",
    "    with open(file_path, 'a', encoding='utf-8') as file:\n",
    "        file.write(content.strip() + '\\n')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = split_arabic_sentences(cleaned_dataset)\n",
    "\n",
    "# Check if the file exists and if it does, delete its content\n",
    "\n",
    "if os.path.exists('Delivery/sentences.txt'):\n",
    "    hf.write_file('Delivery/sentences.txt', '')\n",
    "\n",
    "\n",
    "for s in sentences:\n",
    "    append_to_file('Delivery/sentences.txt', s)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "diacritized_sentences = split_arabic_sentences(cleaned_dataset_with_diacritics)\n",
    "\n",
    "if os.path.exists('Delivery/diacritized_sentences.txt'):\n",
    "    hf.write_file('Delivery/diacritized_sentences.txt', '')\n",
    "\n",
    "for s in diacritized_sentences:\n",
    "    append_to_file('Delivery/diacritized_sentences.txt', s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_sentences = split_arabic_sentences(cleaned_val)\n",
    "\n",
    "if os.path.exists('Delivery/val_sentences.txt'):\n",
    "    hf.write_file('Delivery/val_sentences.txt', '')\n",
    "\n",
    "for s in val_sentences:\n",
    "  append_to_file('Delivery/val_sentences.txt',s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_val = split_arabic_sentences(cleaned_val_with_diacritics)\n",
    "\n",
    "if os.path.exists('Delivery/d_val.txt'):\n",
    "    hf.write_file('Delivery/d_val.txt', '')\n",
    "\n",
    "for s in d_val:\n",
    "  append_to_file('Delivery/d_val.txt',s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def get_diacritics(text,chars,diacritics2id):\n",
    "    text = list(text)\n",
    "    string = ''\n",
    "    diacritics = []\n",
    "    counter = 0 \n",
    "    for char in text:\n",
    "        if char == ' ':\n",
    "            # print('space ////////////////////////////////////')\n",
    "            # print(f'{string} appeneed to diacritics list space condition')\n",
    "            diacritics.append(diacritics_to_id[string])\n",
    "            diacritics.append(14)\n",
    "            string = ''\n",
    "            counter += 1\n",
    "            continue\n",
    "    \n",
    "        if char not in chars:\n",
    "            # print(f\"diacritic {char}\")\n",
    "            string += char\n",
    "            # print('concatenated string',string)\n",
    "        else:\n",
    "            # print('regular char ',char)\n",
    "            if text[counter - 1] in chars:\n",
    "                # print(f'{string} appeneed to diacritics list counter condition')\n",
    "                diacritics.append(diacritics2id[string])\n",
    "                string = ''\n",
    "            elif string != '':\n",
    "                # print(f'{string} appeneed to diacritics list normal condidition')\n",
    "                diacritics.append(diacritics2id[string])\n",
    "    \n",
    "            string = ''\n",
    "\n",
    "        counter += 1    \n",
    "\n",
    "    return diacritics\n",
    "\n",
    "def get_tashkeel(text,char,diacritics2id):\n",
    "    diacritics = get_diacritics(text,char,diacritics2id)\n",
    "    if text[-1] not in char:\n",
    "        diacritics.append(diacritics2id[text[-1]])\n",
    "    return diacritics\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "chars = sorted(arabic_letters) \n",
    "char_to_idx = {char: idx + 1 for idx, char in enumerate(chars)}  # Assigning 0 for padding\n",
    "char_to_idx[' '] = 0\n",
    "idx_to_char = {idx: char for char, idx in char_to_idx.items()}\n",
    "idx_to_char[0] = ' ' \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TashkeelDataset(torch.utils.data.Dataset):\n",
    "\n",
    "  def __init__(self, x, y, pad):\n",
    "    \"\"\"\n",
    "    This is the constructor of the NERDataset\n",
    "    Inputs:\n",
    "    - x: a list of lists where each list contains the ids of the tokens\n",
    "    - y: a list of lists where each list contains the label of each token in the sentence\n",
    "    - pad: the id of the <PAD> token (to be used for padding all sentences and labels to have the same length)\n",
    "    \"\"\"\n",
    "    max_size = max([len(i) for i in x]) # find the max length of the sentences \n",
    "    print('The max size is', max_size)  \n",
    "    self.x = torch.tensor([i + [pad] * (max_size - len(i)) for i in x]) # pad the sentences with <PAD> token\n",
    "    self.y = torch.tensor([i + [14] * (max_size - len(i)) for i in y]) # pad the labels with <PAD> token\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.x)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    return (self.x[idx], self.y[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "gomal = hf.read_file('Delivery/sentences.txt')\n",
    "gomal_tashkeel = hf.read_file('Delivery/diacritized_sentences.txt')\n",
    "\n",
    "x = []\n",
    "for line in gomal.splitlines():\n",
    "    x.append([char_to_idx[char] for char in line])\n",
    "\n",
    "y = []\n",
    "for line in gomal_tashkeel.splitlines():\n",
    "    y.append(get_tashkeel(line,chars,diacritics_to_id))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_gomal = hf.read_file('Delivery/val_sentences.txt')\n",
    "val_gomal_tashkeel = hf.read_file('Delivery/d_val.txt')\n",
    "\n",
    "x_val = []\n",
    "for line in val_gomal.splitlines():\n",
    "    x_val.append([char_to_idx[char] for char in line])\n",
    "\n",
    "y_val = []\n",
    "for line in val_gomal_tashkeel.splitlines():\n",
    "    y_val.append(get_tashkeel(line,chars,diacritics_to_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 5\n",
    "mini_sentences = x[0: 8]\n",
    "mini_labels = y[0: 8]\n",
    "mini_dataset = TashkeelDataset(mini_sentences, mini_labels, 40)\n",
    "dummy_dataloader = torch.utils.data.DataLoader(mini_dataset, batch_size=5)\n",
    "dg = iter(dummy_dataloader)\n",
    "X1, Y1 = next(dg)\n",
    "X2, Y2 = next(dg)\n",
    "print(Y1.shape, X1.shape, Y2.shape, X2.shape)\n",
    "print(X1[0][:], \"\\n\", Y1[0][:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tashkeel(nn.Module):\n",
    "  def __init__(self, vocab_size=37, embedding_dim=37, hidden_size=50, n_classes=15):\n",
    "    \"\"\"\n",
    "    character level tashkeel model\n",
    "    The constructor of our NER model\n",
    "    Inputs:\n",
    "    - vocab_size: the number of unique characters in the dataset\n",
    "    - embedding_dim: the embedding dimension\n",
    "    - n_classes: the number of final classes (tags)\n",
    "    \"\"\"\n",
    "    super(Tashkeel, self).__init__()\n",
    "    self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "    self.lstm = nn.LSTM(embedding_dim, hidden_size, batch_first=True,bidirectional=True)\n",
    "    self.linear = nn.Linear(100, n_classes)\n",
    "\n",
    "  def forward(self, sentences):\n",
    "    \"\"\"\n",
    "    This function does the forward pass of our model\n",
    "    Inputs:\n",
    "    - sentences: tensor of shape (batch_size, max_length)\n",
    "\n",
    "    Returns:\n",
    "    - final_output: tensor of shape (batch_size, max_length, n_classes)\n",
    "    \"\"\"\n",
    "    final_output = self.embedding(sentences)\n",
    "    final_output, _ = self.lstm(final_output)\n",
    "    final_output = self.linear(final_output)\n",
    "    return final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tashkeel(\n",
      "  (embedding): Embedding(37, 37)\n",
      "  (lstm): LSTM(37, 50, batch_first=True, bidirectional=True)\n",
      "  (linear): Linear(in_features=100, out_features=15, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = Tashkeel()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_dataset, batch_size=10, epochs=5, learning_rate=0.01):\n",
    "  \"\"\"\n",
    "  This function implements the training logic\n",
    "  Inputs:\n",
    "  - model: the model ot be trained\n",
    "  - train_dataset: the training set of type NERDataset\n",
    "  - batch_size: integer represents the number of examples per step\n",
    "  - epochs: integer represents the total number of epochs (full training pass)\n",
    "  - learning_rate: the learning rate to be used by the optimizer\n",
    "  \"\"\"\n",
    "  train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=5, shuffle=True)\n",
    "  criterion = nn.CrossEntropyLoss()\n",
    "  optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "  # GPU configuration\n",
    "  use_cuda = torch.cuda.is_available()\n",
    "  device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "  if use_cuda:\n",
    "    model = model.cuda()\n",
    "    criterion = criterion.cuda()\n",
    "\n",
    "  for epoch_num in range(epochs):\n",
    "    total_acc_train = 0\n",
    "    total_loss_train = 0\n",
    "\n",
    "    for train_input, train_label in tqdm(train_dataloader):\n",
    "      train_label = train_label.to(device)\n",
    "      train_input = train_input.to(device)\n",
    "      output = model(train_input)\n",
    "      \n",
    "      batch_loss = criterion(output.view(-1, 15), train_label.view(-1))\n",
    "      total_loss_train += batch_loss.item()\n",
    "\n",
    "      acc = (output.argmax(dim=2) == train_label).sum().item()\n",
    "      total_acc_train += acc\n",
    "      optimizer.zero_grad()\n",
    "      batch_loss.backward()\n",
    "      optimizer.step()\n",
    "      \n",
    "    epoch_loss = total_loss_train / len(train_dataset)\n",
    "    epoch_acc = total_acc_train / (len(train_dataset) * len(train_dataset[0][0]))\n",
    "    print(\n",
    "        f'Epochs: {epoch_num + 1} | Train Loss: {epoch_loss} \\\n",
    "        | Train Accuracy: {epoch_acc}\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The max size is 2544\n",
      "The max size is 1739\n"
     ]
    }
   ],
   "source": [
    "train_dataset = TashkeelDataset(x[0:1000], y[0:1000], 0)\n",
    "val_dataset = TashkeelDataset(x_val[0:500], y_val[0:500], 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(model, train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_evaluation(model,test_dataset,batch_size=5):\n",
    "     # Create the test data loader\n",
    "    test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "    # GPU Configuration\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "    if use_cuda:\n",
    "        model = model.cuda()\n",
    "\n",
    "    total_correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "    letter_count = 0\n",
    "    correct_letter_d = 0\n",
    "    all_predictions = []\n",
    "    all_test_inputs = []\n",
    "    with torch.no_grad():\n",
    "        for test_input, test_label in tqdm(test_dataloader):\n",
    "            # Move the test input to the device\n",
    "            test_label = test_label.to(device)\n",
    "            # Move the test label to the device\n",
    "            test_input = test_input.to(device)\n",
    "\n",
    "            # Perform the forward pass\n",
    "            output = model(test_input)\n",
    "\n",
    "            # Get predicted labels\n",
    "            predicted_labels = output.argmax(dim=2)\n",
    "            # print(test_input.shape)\n",
    "            for j,batch in enumerate(test_input):\n",
    "\n",
    "              for i,char in enumerate(batch):\n",
    "                # print(char.shape)\n",
    "                if char.item() != 0:\n",
    "                  letter_count += 1\n",
    "                  if predicted_labels[j][i].item() == test_label[j][i].item():\n",
    "                    correct_letter_d += 1\n",
    "\n",
    "            all_test_inputs.append(test_input)\n",
    "            all_predictions.append(predicted_labels)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            # Count correct predictions, excluding label 15\n",
    "            # correct_predictions = ((predicted_labels == test_label) & (test_label != 15)).sum().item()\n",
    "            # total_correct_predictions += correct_predictions\n",
    "\n",
    "            # # Count total predictions, excluding instances where the ground truth label is 15\n",
    "            # valid_predictions = (test_label != 15).sum().item()\n",
    "            # total_predictions += valid_predictions\n",
    "            # space_pred = (predicted_labels == 14).sum().item()\n",
    "            # total_correct_predictions +=space_pred\n",
    "            # valid_predictions = (test_label == 14).sum().item()\n",
    "            # total_predictions += valid_predictions\n",
    "\n",
    "\n",
    "\n",
    "    # Calculate the overall accuracy excluding label 15\n",
    "    overall_accuracy = correct_letter_d / letter_count\n",
    "    print(len(all_test_inputs),len(all_predictions))\n",
    "    print(f'overall accuracy: {overall_accuracy}')\n",
    "    return all_test_inputs,all_predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_inputs,predictions = correct_evaluation(model,val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data_from_id(test_input, predictions, id_to_diacritics, idx_to_char):\n",
    "  for k,test in enumerate(test_input):\n",
    "    for j,batch in enumerate(test):\n",
    "        string = ''\n",
    "        for i,char in enumerate(batch):\n",
    "          # print(char.shape)\n",
    "          # print(\"len \", len(predictions))\n",
    "          # print(\"shape \", predictions[k].shape)\n",
    "          # print(\"prediction \",predictions[k][j][i])\n",
    "          string += (idx_to_char[test[j][i].item()] + id_to_diacritics[predictions[k][j][i].item()])\n",
    "        print(string)\n",
    "        break\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "قَوْلُهُ وَلَا تُكْرَهُ ضِيَافَتُهُ\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:06<00:00, 16.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 100\n",
      "overall accuracy: 0.7410149770416136\n",
      "قَوْلُهُ وَلَا تُكْرْهُ ضِيَافَتُهُ                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"قَوْلُهُ وَلَا تُكْرَهُ ضِيَافَتُهُ\")\n",
    "\n",
    "id_to_diacritics = {}\n",
    "for key, value in diacritics_to_id.items():\n",
    "    id_to_diacritics[value] = key\n",
    "\n",
    "# pklmodel = torch.load('Delivery/model.pickle')\n",
    "# test_inputs,predictions = correct_evaluation(pklmodel,val_dataset)\n",
    "\n",
    "extract_data_from_id(test_inputs, predictions, id_to_diacritics, idx_to_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
